{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ff37c92-6ef5-496f-9623-54f1f956bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "406ac481-5b23-4260-8ea3-b1310b766196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43e11997-6a3e-4e88-ac38-48d828b0eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ai_articles(soup):\n",
    "    articles = soup.find_all('article', class_='post-block')\n",
    "    print(f\"Found {len(articles)} articles in total\")\n",
    "    ai_articles = []\n",
    "    for article in articles:\n",
    "        title_elem = article.find('h2', class_='post-block__title')\n",
    "        if title_elem:\n",
    "            title = title_elem.text.strip()\n",
    "            if 'ai' in title.lower() or 'artificial intelligence' in title.lower():\n",
    "                link_elem = article.find('a', class_='post-block__title__link')\n",
    "                excerpt_elem = article.find('div', class_='post-block__content')\n",
    "                if link_elem and excerpt_elem:\n",
    "                    link = link_elem['href']\n",
    "                    excerpt = excerpt_elem.text.strip()\n",
    "                    ai_articles.append({'title': title, 'link': link, 'excerpt': excerpt})\n",
    "    print(f\"Found {len(ai_articles)} AI-related articles\")\n",
    "    return ai_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e3901b2-73be-417d-a465-a51faff3e1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 articles in total\n",
      "Found 0 AI-related articles\n",
      "No AI-related articles found.\n"
     ]
    }
   ],
   "source": [
    "# Scrape AI-related news from TechCrunch\n",
    "url_scrape = \"https://techcrunch.com/\"\n",
    "html_doc = scrape(url_scrape)\n",
    "if html_doc:\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "    # Get AI-related articles\n",
    "    ai_articles = get_ai_articles(soup)\n",
    "\n",
    "    if ai_articles:\n",
    "        # Initialize summarizer with bart-large-cnn for summarization\n",
    "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "        # Summarize articles\n",
    "        summaries = []\n",
    "        for article in ai_articles:\n",
    "            print(f\"Processing article: {article['title']}\")\n",
    "            article_html = scrape(article['link'])\n",
    "            if article_html:\n",
    "                article_soup = BeautifulSoup(article_html, 'html.parser')\n",
    "                article_content = article_soup.find('div', class_='article-content')\n",
    "                if article_content:\n",
    "                    article_text = article_content.text.strip()\n",
    "                    summary = summarize_text(article_text, summarizer)\n",
    "                    if summary:\n",
    "                        summaries.append({\n",
    "                            'title': article['title'],\n",
    "                            'link': article['link'],\n",
    "                            'summary': summary\n",
    "                        })\n",
    "                else:\n",
    "                    print(f\"Couldn't find article content for {article['title']}\")\n",
    "            else:\n",
    "                print(f\"Couldn't scrape article page for {article['title']}\")\n",
    "\n",
    "        # Create a DataFrame with the results\n",
    "        df = pd.DataFrame(summaries)\n",
    "\n",
    "        # Display the results\n",
    "        if not df.empty:\n",
    "            print(df.to_string(index=False))\n",
    "        else:\n",
    "            print(\"No summaries were generated.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No AI-related articles found.\")\n",
    "else:\n",
    "    print(\"Failed to scrape the TechCrunch homepage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce547a28-ea9a-44f5-8a42-c08f94691908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Widen search, UI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5b1c8-04c8-4d19-a04a-37e459fc48e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
